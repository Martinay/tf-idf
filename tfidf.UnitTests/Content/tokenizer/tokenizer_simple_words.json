{"class_name": "Tokenizer", "config": {"num_words": 6, "filters": "", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 3, "word_counts": "{\"hallo\": 3, \"dies\": 1, \"ist\": 2, \"ein\": 2, \"test\": 1}", "word_docs": "{\"hallo\": 3, \"dies\": 1, \"ist\": 2, \"ein\": 2, \"test\": 1}", "index_docs": "{\"1\": 3, \"4\": 1, \"2\": 2, \"3\": 2, \"5\": 1}", "index_word": "{\"1\": \"hallo\", \"2\": \"ist\", \"3\": \"ein\", \"4\": \"dies\", \"5\": \"test\"}", "word_index": "{\"hallo\": 1, \"ist\": 2, \"ein\": 3, \"dies\": 4, \"test\": 5}"}}
